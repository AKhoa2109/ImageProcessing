import streamlit as st
import cv2
import numpy as np
import mediapipe as mp
from typing import List, Tuple

# C·∫•u h√¨nh MediaPipe
mp_face_mesh = mp.solutions.face_mesh

# ƒê·ªãnh nghƒ©a k·∫øt n·ªëi ƒë·ªÉ v·∫Ω
LEFT_EYE_CONNECTIONS = mp_face_mesh.FACEMESH_LEFT_EYE
RIGHT_EYE_CONNECTIONS = mp_face_mesh.FACEMESH_RIGHT_EYE

# ƒê·ªãnh nghƒ©a ch·ªâ s·ªë landmark ƒë·ªÉ t√≠nh EAR
LEFT_EYE_LANDMARKS = [33, 160, 158, 133, 153, 144]
RIGHT_EYE_LANDMARKS = [362, 385, 387, 263, 373, 380]

# H·∫±ng s·ªë
EAR_THRESHOLD = 0.21  # Ng∆∞·ª°ng t·ª∑ l·ªá m·∫Øt

def calculate_ear(eye_idxs: List[int], landmarks: List[Tuple[float, float]]) -> float:
    """T√≠nh to√°n t·ª∑ l·ªá khung m·∫Øt (Eye Aspect Ratio)"""
    p0, p1, p2, p3, p4, p5 = [np.array(landmarks[i]) for i in eye_idxs]
    vertical1 = np.linalg.norm(p1 - p5)
    vertical2 = np.linalg.norm(p2 - p4)
    horizontal = np.linalg.norm(p0 - p3)
    return (vertical1 + vertical2) / (2.0 * horizontal)

def process_frame(image):
    """X·ª≠ l√Ω frame ·∫£nh ƒë·ªÉ ph√°t hi·ªán ch·ªõp m·∫Øt v√† v·∫Ω landmarks"""
    h, w = image.shape[:2]
    with mp_face_mesh.FaceMesh(
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    ) as face_mesh:
        rgb_frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_frame)
        if results.multi_face_landmarks:
            lm = results.multi_face_landmarks[0]
            landmarks = [(p.x, p.y) for p in lm.landmark]
            left_ear = calculate_ear(LEFT_EYE_LANDMARKS, landmarks)
            right_ear = calculate_ear(RIGHT_EYE_LANDMARKS, landmarks)
            avg_ear = (left_ear + right_ear) / 2.0

            eye_closed = avg_ear < EAR_THRESHOLD

            # Ph√°t hi·ªán chuy·ªÉn ƒë·ªïi tr·∫°ng th√°i m·∫Øt
            if eye_closed and not st.session_state.previous_closed:
                st.session_state.eye_closed = True
            elif not eye_closed and st.session_state.previous_closed:
                st.session_state.blink_counter += 1
                st.session_state.eye_closed = False

            st.session_state.previous_closed = eye_closed

            # V·∫Ω k·∫øt n·ªëi m·∫Øt
            for connection in [LEFT_EYE_CONNECTIONS, RIGHT_EYE_CONNECTIONS]:
                for start, end in connection:
                    x1 = int(lm.landmark[start].x * w)
                    y1 = int(lm.landmark[start].y * h)
                    x2 = int(lm.landmark[end].x * w)
                    y2 = int(lm.landmark[end].y * h)
                    cv2.line(image, (x1, y1), (x2, y2), (0, 255, 0), 1)
        return image

def main():
    st.title("Ph√°t hi·ªán ch·ªõp m·∫Øt th·ªùi gian th·ª±c üëÅÔ∏è")

    # Kh·ªüi t·∫°o session state
    if 'blink_counter' not in st.session_state:
        st.session_state.blink_counter = 0
    if 'previous_closed' not in st.session_state:
        st.session_state.previous_closed = False
    if 'eye_closed' not in st.session_state:
        st.session_state.eye_closed = False

    start = st.button("B·∫Øt ƒë·∫ßu ph√°t hi·ªán")
    stop = st.button("D·ª´ng")

    if start:
        cap = cv2.VideoCapture(0)
        img_placeholder = st.empty()
        count_placeholder = st.empty()
        status_placeholder = st.empty()

        while cap.isOpened():
            if stop:
                break
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.flip(frame, 1)
            processed = process_frame(frame)

            # Hi·ªÉn th·ªã h√¨nh ·∫£nh v√† th√¥ng tin
            img_placeholder.image(processed, channels="BGR", use_container_width=True)
            count_placeholder.markdown(f"**S·ªë l·∫ßn ch·ªõp m·∫Øt:** {st.session_state.blink_counter}")
            status = "ƒê√≥ng m·∫Øt" if st.session_state.eye_closed else "M·ªü m·∫Øt"
            status_placeholder.markdown(f"**Tr·∫°ng th√°i:** {status}")

        cap.release()

if __name__ == "__main__":
    main()